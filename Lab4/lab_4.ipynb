{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh_AUmrgHrdb"
      },
      "source": [
        "# Language Detection Classifier Lab: Implementing a Compact Language Detector (Inspired by Google's CLD3)\n",
        "\n",
        "**Welcome to Lab 4!** This lab guides you through building a language classifier using n-gram features and the \"hashing trick\" for efficient feature representation. We'll reimplement key ideas from Google's Compact Language Detector (CLD3) using the Tatoeba dataset (downsampled for this lab).\n",
        "\n",
        "### Learning Objectives\n",
        "- Understand n-gram extraction and feature hashing for text classification.\n",
        "- Implement data processing pipelines (unigrams, bigrams, trigrams ‚Üí hashed vectors).\n",
        "- Train and evaluate baseline models: Logistic Regression and a simple Neural Network.\n",
        "- Experiment with encodings: Boolean (presence) vs. Relative Frequencies.\n",
        "- (Optional) Extend to learnable embeddings.\n",
        "\n",
        "### Dataset\n",
        "- **Source**: Tatoeba (sentences + translations, downsampled to ~31k train, ~4k val/test sentences across 39 languages).\n",
        "- **Format**: TSV files (`train.tsv`, `val.tsv`, `test.tsv`) with columns: [ID, Language Code, Sentence].\n",
        "- **Access**: The notebook auto-downloads from the course GitHub repo if not local.\n",
        "\n",
        "### Setup Instructions\n",
        "1. Clone the repo: `git clone https://github.com/Azadshokrollahi/Artificial-intelligence-for-data-science-.git`\n",
        "2. Navigate: `cd Lab4`\n",
        "3. Run: `jupyter notebook language_detector_student.ipynb`\n",
        "4. The notebook handles dataset fetching automatically.\n",
        "\n",
        "**Run the cells step-by-step. Fill in the blanks marked with `# TODO: Student Task` and test your implementations.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a5cfgh5Hrdc",
        "outputId": "496e94eb-c5eb-49de-f0d9-3a4960b062a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Local small_dataset not found. Downloading from GitHub repo...\n",
            "  ‚Ü≥ train.tsv ‚Üê https://raw.githubusercontent.com/Azadshokrollahi/Artificial-intelligence-for-data-science-/main/Lab4/small_dataset/train.tsv\n",
            "  ‚Ü≥ val.tsv ‚Üê https://raw.githubusercontent.com/Azadshokrollahi/Artificial-intelligence-for-data-science-/main/Lab4/small_dataset/val.tsv\n",
            "  ‚Ü≥ test.tsv ‚Üê https://raw.githubusercontent.com/Azadshokrollahi/Artificial-intelligence-for-data-science-/main/Lab4/small_dataset/test.tsv\n",
            "[INFO] Using small dataset at: /content/small_dataset\n",
            "       Train: small_dataset/train.tsv\n",
            "       Val  : small_dataset/val.tsv\n",
            "       Test : small_dataset/test.tsv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c1836ff1930>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# === Dataset Locator (Auto-download from GitHub if needed) ===\n",
        "from pathlib import Path\n",
        "import os\n",
        "import urllib.request  # For downloading if needed\n",
        "\n",
        "FILENAME_TRAIN = \"train.tsv\"\n",
        "FILENAME_VAL   = \"val.tsv\"\n",
        "FILENAME_TEST  = \"test.tsv\"\n",
        "\n",
        "# Candidate locations if you run the notebook from different folders\n",
        "CANDIDATES = [\n",
        "    Path(\"small_dataset\"),\n",
        "    Path(\"Lab4\") / \"small_dataset\",\n",
        "    Path.cwd() / \"small_dataset\",\n",
        "    Path.cwd() / \"Lab4\" / \"small_dataset\",\n",
        "]\n",
        "\n",
        "dataset_dir = next((p for p in CANDIDATES if (p / FILENAME_TRAIN).exists()\n",
        "                    and (p / FILENAME_VAL).exists()\n",
        "                    and (p / FILENAME_TEST).exists()), None)\n",
        "\n",
        "# If not found locally, download directly from Azad's GitHub repo into ./small_dataset\n",
        "if dataset_dir is None:\n",
        "    print(\"[INFO] Local small_dataset not found. Downloading from GitHub repo...\")\n",
        "    dataset_dir = Path(\"small_dataset\")\n",
        "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    base = \"https://raw.githubusercontent.com/Azadshokrollahi/Artificial-intelligence-for-data-science-/main/Lab4/small_dataset\"\n",
        "    for fname in [FILENAME_TRAIN, FILENAME_VAL, FILENAME_TEST]:\n",
        "        url = f\"{base}/{fname}\"\n",
        "        out = dataset_dir / fname\n",
        "        print(f\"  ‚Ü≥ {fname} ‚Üê {url}\")\n",
        "        urllib.request.urlretrieve(url, out)\n",
        "\n",
        "# Final resolved files\n",
        "FILE_TRAIN = str(dataset_dir / FILENAME_TRAIN)\n",
        "FILE_VAL   = str(dataset_dir / FILENAME_VAL)\n",
        "FILE_TEST  = str(dataset_dir / FILENAME_TEST)\n",
        "\n",
        "print(\"[INFO] Using small dataset at:\", dataset_dir.resolve())\n",
        "print(\"       Train:\", FILE_TRAIN)\n",
        "print(\"       Val  :\", FILE_VAL)\n",
        "print(\"       Test :\", FILE_TEST)\n",
        "\n",
        "# === Preliminaries ===\n",
        "import random\n",
        "import hashlib\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(4321)\n",
        "torch.manual_seed(4321)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0xRDaWnHrdd"
      },
      "source": [
        "## Cell 2: Settings and Language Indexing\n",
        "Define hyperparameters and create language-to-index mappings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqSKgFtkHrdd",
        "outputId": "65acf04a-edc8-481e-db61-1da2738b683e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Detected 39 languages: ['ara', 'ber', 'bul', 'ces', 'cmn']...\n",
            "Sample idx2lang: {0: 'ara', 1: 'ber', 2: 'bul', 3: 'ces', 4: 'cmn'}\n",
            "Sample lang2idx: {'ara': 0, 'ber': 1, 'bul': 2, 'ces': 3, 'cmn': 4}\n"
          ]
        }
      ],
      "source": [
        "# Settings\n",
        "SMALL_DATASET_PATH = 'small_dataset'  # Already resolved above\n",
        "\n",
        "REL_FREQ = True  # TODO: Experiment: Set to False for Boolean encoding (presence only)\n",
        "HIDDEN_LAYER = False  # TODO: Experiment: Set to True for Neural Network with hidden layer\n",
        "HIDDEN_DIM = 512  # Hidden layer size (only used if HIDDEN_LAYER=True)\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "dataset_path = SMALL_DATASET_PATH\n",
        "FILE_TRAIN = dataset_path + '/' + FILENAME_TRAIN\n",
        "FILE_VAL = dataset_path + '/' + FILENAME_VAL\n",
        "FILE_TEST = dataset_path + '/' + FILENAME_TEST\n",
        "\n",
        "# File reader generator\n",
        "def file_reader(file):\n",
        "    with open(file, encoding='utf8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            row = line.strip()\n",
        "            yield tuple(row.split('\\t'))\n",
        "\n",
        "# Count languages in train\n",
        "line_generator = file_reader(FILE_TRAIN)\n",
        "lang_freqs = Counter(map(lambda x: x[1], line_generator))\n",
        "langs = sorted(list(set(lang_freqs.keys())))\n",
        "\n",
        "# TODO============================== Student Task 1 - Create idx2lang: A dictionary mapping indices (0 to len(langs)-1) to language codes\n",
        "# Example: {0: 'ara', 1: 'ber', ...}\n",
        " # Fill in your code here\n",
        "idx2lang = {}\n",
        "for i, lang in enumerate(langs):\n",
        "    idx2lang[i] = lang\n",
        "\n",
        "# TODO==============================: Student Task 2 - Create lang2idx: The reverse mapping (language code to index)\n",
        "# Example: {'ara': 0, 'ber': 1, ...}\n",
        "# Fill in your code here\n",
        "lang2idx = {}\n",
        "for i, lang in idx2lang.items():\n",
        "    lang2idx[lang] = i\n",
        "\n",
        "print(f\"[INFO] Detected {len(langs)} languages: {langs[:5]}...\")\n",
        "print(\"Sample idx2lang:\", dict(list(idx2lang.items())[:5]))\n",
        "print(\"Sample lang2idx:\", dict(list(lang2idx.items())[:5]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lb6hy_WHrdd"
      },
      "source": [
        "## Cell 3: N-Gram Extraction and Hashing\n",
        "Extract unigrams/bigrams/trigrams and hash them into fixed-size indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ABWbwqdHrdd",
        "outputId": "1ed7fad3-db1d-4142-b7de-f6b534167205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test ngrams('banana', 2): ['ba', 'an', 'na', 'an', 'na']\n",
            "Test all_ngrams('banana'): [['b', 'a', 'n', 'a', 'n', 'a'], ['ba', 'an', 'na', 'an', 'na'], ['ban', 'ana', 'nan', 'ana']]\n",
            "[INFO] Feature space size: 2583 (chars: 521, bigrams: 1031, trigrams: 1031)\n",
            "Test hash_ngrams('banana'): [[25, 234, 310, 234, 310, 234], [994, 649, 808, 649, 808]]\n"
          ]
        }
      ],
      "source": [
        "# N-grams extraction\n",
        "# TODO: Student Task - Implement ngrams: Extract all n-length substrings from sentence.\n",
        "# Args: sentence (str), n (int=1), lc (bool=True) - lowercase if True.\n",
        "# Return: List of n-grams (str), e.g., ngrams('banana', 2) -> ['ba', 'an', 'na', 'an', 'na']\n",
        "# Hint: Use a loop over range(len(s) - n + 1), slice s[i:i+n]\n",
        "def ngrams(sentence, n=1, lc=True):\n",
        "    ngram_l = []\n",
        "    s = sentence.lower() if lc else sentence\n",
        "    # Task 3============================ TODO: Your code here\n",
        "    substrList = {}\n",
        "    for i in range(len(sentence) - n+1):\n",
        "      substrList[i] = sentence[i:i+n]\n",
        "    return list(substrList.values())\n",
        "\n",
        "\n",
        "\n",
        "def all_ngrams(sentence, max_ngram=3, lc=True):\n",
        "    all_ngram_list = []\n",
        "    for i in range(1, max_ngram + 1):\n",
        "        all_ngram_list += [ngrams(sentence, n=i, lc=lc)]\n",
        "    return all_ngram_list\n",
        "\n",
        "# Test your implementation\n",
        "print(\"Test ngrams('banana', 2):\", ngrams('banana', 2))\n",
        "print(\"Test all_ngrams('banana'):\", all_ngrams('banana'))\n",
        "# Expected: [['b', 'a', 'n', 'a', 'n', 'a'], ['ba', 'an', 'na', 'an', 'na'], ['ban', 'ana', 'nan', 'ana']]\n",
        "\n",
        "# Hashing\n",
        "def reproducible_hash(string):\n",
        "    h = hashlib.md5(string.encode(\"utf-8\"), usedforsecurity=False)\n",
        "    return int.from_bytes(h.digest()[0:8], 'big', signed=True)\n",
        "\n",
        "# Modulos for feature space (Fixed to small dataset values)\n",
        "MAX_CHARS = 521\n",
        "MAX_BIGRAMS = 1031\n",
        "MAX_TRIGRAMS = 1031\n",
        "NUM_FEATURES = MAX_CHARS + MAX_BIGRAMS + MAX_TRIGRAMS\n",
        "MAXES = [MAX_CHARS, MAX_BIGRAMS, MAX_TRIGRAMS]\n",
        "\n",
        "print(f\"[INFO] Feature space size: {NUM_FEATURES} (chars: {MAX_CHARS}, bigrams: {MAX_BIGRAMS}, trigrams: {MAX_TRIGRAMS})\")\n",
        "\n",
        "# TODO: Student Task - Implement hash_ngrams: Hash each n-gram list into indices modulo respective MAX.\n",
        "# Args: ngram_lists (list of 3 lists: unigrams, bigrams, trigrams), modulos (list of 3 MAX values).\n",
        "# Return: List of 3 lists: hashed indices, e.g., [[hash('b') % 521, ...], ...]\n",
        "# Hint: Loop over zip(ngram_lists, modulos), use list comprehension with reproducible_hash % modulo\n",
        "def hash_ngrams(ngram_lists, modulos):\n",
        "    hash_codes = []\n",
        "    # Task 4============================== TODO: Your code here\n",
        "    for ngram_list, modulo in zip(ngram_lists, modulos):\n",
        "      hash_codes.append([reproducible_hash(x) % modulo for x in ngram_list])\n",
        "\n",
        "    return hash_codes\n",
        "\n",
        "\n",
        "# Test\n",
        "hash_banana = hash_ngrams(all_ngrams('banana'), MAXES)\n",
        "print(\"Test hash_ngrams('banana'):\", hash_banana[:2])  # First 2 for brevity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENXU2aP7Hrdd"
      },
      "source": [
        "## Cell 4: Relative Frequencies and Multihot Vectors\n",
        "Compute n-gram frequencies and encode as sparse vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5vQrc6DHrde",
        "outputId": "6eb30ccc-78e4-49c8-8896-2738a672f34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test rel_freqs (chars): {25: 0.16666666666666666, 234: 0.5, 310: 0.3333333333333333}\n",
            "Nonzero indices (multihot): [25, 234, 310]\n",
            "Values at nonzero (multihot_freq): [[0.1666666716337204], [0.5], [0.3333333432674408]]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Student Task - Implement rel_freqs: Compute relative frequencies of items in lst.\n",
        "# Args: lst (list of hashes).\n",
        "# Return: Dict {hash: freq} where freq = count / len(lst), e.g., Counter then normalize.\n",
        "# Hint: Use collections.Counter, then {k: v / total for k,v in counter.items()}\n",
        "def rel_freqs(lst):\n",
        "    #  Task 5========================= TODO: Your code here\n",
        "    counter = Counter(lst)\n",
        "    total = len(lst)\n",
        "    freq = {k: v / total for k,v in counter.items()}\n",
        "    return freq\n",
        "\n",
        "# Test\n",
        "freqs_banana = [rel_freqs(x) for x in hash_banana]\n",
        "print(\"Test rel_freqs (chars):\", freqs_banana[0])\n",
        "# Expected: {25: 0.1667, 234: 0.5, 310: 0.3333, ...}\n",
        "\n",
        "# Multihot vectors (Boolean version)\n",
        "# TODO: Student Task - Implement multihot: Create torch vector of size 'max' with 1.0 at dict keys, else 0.0.\n",
        "# Args: idx_freq (dict, but ignore values), max (int).\n",
        "# Return: torch.zeros(max) with feat_vector[idx] = 1.0 for idx in keys.\n",
        "def multihot(idx_freq, max_size):\n",
        "    feat_vector = torch.zeros(max_size)\n",
        "    #  Task 6=================================== TODO: Your code here\n",
        "    for idx in idx_freq.keys():\n",
        "      if idx < max_size:\n",
        "       feat_vector[idx] = 1.0\n",
        "    return feat_vector\n",
        "\n",
        "# Multihot with frequencies\n",
        "# TODO: Student Task - Implement multihot_freq: Like multihot, but set feat_vector[idx] = freq from dict values.\n",
        "def multihot_freq(idx_freq, max_size):\n",
        "    feat_vector = torch.zeros(max_size)\n",
        "    #  Task 7============================== TODO: Your code here\n",
        "    for idx, freq in idx_freq.items():\n",
        "      if idx < max_size:\n",
        "        feat_vector[idx] = freq\n",
        "    return feat_vector\n",
        "\n",
        "# Test\n",
        "mhot_char = multihot(freqs_banana[0], MAX_CHARS)\n",
        "print(\"Nonzero indices (multihot):\", torch.nonzero(mhot_char).squeeze().tolist())\n",
        "freq_char = multihot_freq(freqs_banana[0], MAX_CHARS)\n",
        "print(\"Values at nonzero (multihot_freq):\", freq_char[torch.nonzero(freq_char)].tolist())\n",
        "# Expected: Indices [25,234,310]; Values [0.1667, 0.5, 0.3333]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqI9yZ2sHrde"
      },
      "source": [
        "## Cell 5: Data Loading and Tensor Creation\n",
        "Load sentences into X (features) and y (labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O42z3WE4Hrde"
      },
      "outputs": [],
      "source": [
        "# Data loader\n",
        "def read_sent_lang(file):\n",
        "    with open(file, encoding='utf8', errors='ignore') as f:\n",
        "        for line in f:\n",
        "            row = line.strip()\n",
        "            lang_tuple = tuple(row.split('\\t'))\n",
        "            yield lang_tuple[2], lang_tuple[1]   # sentence, lang\n",
        "\n",
        "# Process file into X (N x NUM_FEATURES tensor), y (N tensor of indices).\n",
        "# Steps:\n",
        "# 1. Count lines for X size (pre-allocate).\n",
        "# 2. Loop over read_sent_lang: Extract all_ngrams ‚Üí hash_ngrams ‚Üí [rel_freqs each] ‚Üí [multihot_func each] ‚Üí torch.cat ‚Üí X[i]\n",
        "# 3. Map langs to indices with lang2idx ‚Üí torch.LongTensor(y_symb)\n",
        "# Use tqdm for progress. Handle via multihot_func (freq or bool).\n",
        "def create_Xy(file, multihot_func, lang2idx):\n",
        "    line_cnt = 0\n",
        "    for sentence, lang in read_sent_lang(file):\n",
        "        line_cnt += 1\n",
        "    X = torch.empty((line_cnt, NUM_FEATURES))\n",
        "    y_symb = []\n",
        "    for i, (sentence, lang) in tqdm(enumerate(read_sent_lang(file)), total=line_cnt, desc=\"Processing\"):\n",
        "        # TODO: Your code here - extract ngrams, hash, freqs, multihot, cat to X[i]\n",
        "        hashes = hash_ngrams(all_ngrams(sentence), MAXES)\n",
        "        hash_freq_l = list(map(rel_freqs, hashes))\n",
        "        x_row_l = []\n",
        "        for hash_freq_dict, max_val in zip(hash_freq_l, MAXES):\n",
        "            x_row_l += [multihot_func(hash_freq_dict, max_val)]\n",
        "        X[i, :] = torch.cat(x_row_l, -1)\n",
        "        y_symb += [lang]\n",
        "    y = torch.LongTensor(list(map(lang2idx.get, y_symb)))\n",
        "    return X, y\n",
        "\n",
        "# Create datasets\n",
        "multihot_func = multihot_freq if REL_FREQ else multihot\n",
        "X_train, y_train = create_Xy(FILE_TRAIN, multihot_func, lang2idx)\n",
        "X_val, y_val = create_Xy(FILE_VAL, multihot_func, lang2idx)\n",
        "X_test, y_test = create_Xy(FILE_TEST, multihot_func, lang2idx)\n",
        "\n",
        "input_dim = X_train.size()[1]\n",
        "print(f\"[INFO] Dataset shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "print(\"Sample y_train:\", y_train[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76sF1N6NHrde"
      },
      "source": [
        "## Cell 6: Model Architectures\n",
        "Define Logistic Regression or Neural Net."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-LPi6PoHrde"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "# TODO: Student Task - Define two architectures using nn.Sequential.\n",
        "# If HIDDEN_LAYER: Linear(input_dim ‚Üí HIDDEN_DIM) ‚Üí ReLU ‚Üí Linear(HIDDEN_DIM ‚Üí num_langs)\n",
        "# Else: Linear(input_dim ‚Üí num_langs)  # Logistic Regression\n",
        "if HIDDEN_LAYER:\n",
        "    model = nn.Sequential(\n",
        "        #  Task 8 ====================TODO: Your code here\n",
        "\n",
        "    )\n",
        "else:\n",
        "    model = nn.Sequential(\n",
        "        #  Task 9====================TODO: Your code here\n",
        "\n",
        "    )\n",
        "\n",
        "print(f\"[INFO] Model: {'Neural Network (hidden dim=' + str(HIDDEN_DIM) + ')' if HIDDEN_LAYER else 'Logistic Regression'}\")\n",
        "print(f\"Output classes: {len(langs)}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=0.01)\n",
        "\n",
        "# DataLoader for batches\n",
        "dataset = TensorDataset(X_train, y_train)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kexd9_uHrdf"
      },
      "source": [
        "## Cell 7: Training Loop\n",
        "Train the model with validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L9PV587Hrdf"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "loss_train_history = []\n",
        "acc_train_history = []\n",
        "loss_val_history = []\n",
        "acc_val_history = []\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Training\"):\n",
        "    # Training phase\n",
        "    loss_train_epoch = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    model.train()  # Enable dropout/etc. (if any)\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = loss_fn(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_train_epoch += loss.item() * len(X_batch)\n",
        "        _, predicted = torch.max(y_pred.data, 1)\n",
        "        total_train += len(y_batch)\n",
        "        correct_train += (predicted == y_batch).sum().item()\n",
        "\n",
        "    avg_loss_train = loss_train_epoch / len(dataloader.dataset)\n",
        "    acc_train = correct_train / total_train\n",
        "\n",
        "    # Task 10======================TODO: Student Task - Add validation phase: model.eval(), no_grad(), predict on X_val, compute loss/acc.\n",
        "    # Append to histories. Print every 2 epochs.\n",
        "    # Hint: Reuse train logic but without gradients/optimizer\n",
        "    model.eval()\n",
        "    #your code\n",
        "\n",
        "\n",
        "\n",
        "    loss_train_history.append(avg_loss_train)\n",
        "    acc_train_history.append(acc_train)\n",
        "    loss_val_history.append(loss_val)\n",
        "    acc_val_history.append(acc_val)\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Train Acc: {acc_train:.4f}, Val Acc: {acc_val:.4f}\")\n",
        "\n",
        "# Plot\n",
        "epochs_range = range(1, len(acc_val_history) + 1)\n",
        "plt.plot(epochs_range, acc_val_history, 'bo-', label='Validation accuracy')\n",
        "plt.title('Validation Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er_97fHiHrdf"
      },
      "source": [
        "## Cell 8: Evaluation and Inference\n",
        "Test the model and demo detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtIpLuaAHrdf"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # TODO: Student Task - Predict: y_test_pred = argmax(model(X_test), dim=1)\n",
        "    y_test_pred = torch.argmax(model(X_test), dim=1)  # Your code here\n",
        "\n",
        "print(\"\\n=== Test Results ===\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=langs, digits=4))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "# Detector function (for new text)\n",
        "def encode(text, multihot_func, MAXES):\n",
        "    hashes = hash_ngrams(all_ngrams(text), MAXES)\n",
        "    hash_freq_l = list(map(rel_freqs, hashes))\n",
        "    x_row = torch.empty((0,))\n",
        "    for hash_freq_dict, max_val in zip(hash_freq_l, MAXES):\n",
        "        x_row = torch.hstack((x_row, multihot_func(hash_freq_dict, max_val)))\n",
        "    return x_row\n",
        "\n",
        "# Example predictions\n",
        "print(\"\\n=== Example Predictions ===\")\n",
        "test_sents = ['Hi guys and girls!', 'Hur m√•r du nu?', 'Allt bra idag?', 'Salut tout le monde !']\n",
        "for sent in test_sents:\n",
        "    row = encode(sent, multihot_func, MAXES)\n",
        "    pred_lang = idx2lang[torch.argmax(model(row), dim=-1).item()]\n",
        "    print(f\"'{sent}' ‚Üí {pred_lang}\")\n",
        "\n",
        "# Verification: Instructor example (Swedish: 'swe')\n",
        "example_sent = \"Stanna!\"\n",
        "row_example = encode(example_sent, multihot_func, MAXES)\n",
        "pred_example = idx2lang[torch.argmax(model(row_example), dim=-1).item()]\n",
        "print(f\"\\nVerification: 'Stanna!' ‚Üí {pred_example} (expected: swe)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb7fNxR0Hrdf"
      },
      "source": [
        "## Cell 9: Results and Experiments\n",
        "Compute F1 and fill table. Rerun with toggles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVaMfzulHrdf"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "y_val_pred = torch.argmax(model(X_val), dim=1)\n",
        "macro_f1_val = f1_score(y_val, y_val_pred, average='macro')\n",
        "macro_f1_test = f1_score(y_test, y_test_pred, average='macro')\n",
        "print(f\"\\nCurrent Config - Macro F1 Val: {macro_f1_val:.4f}, Test: {macro_f1_test:.4f}\")\n",
        "\n",
        "# TODO: Student Task 11==================================== - Rerun 4 configs (REL_FREQ True/False x HIDDEN_LAYER True/False).\n",
        "# Fill table below with your Macro F1 scores. Discuss trends.\n",
        "\n",
        "print(\"\\n=== Results Table ===\")\n",
        "print(\"| Method              | Encoding    | Macro F1: Val | Macro F1: Test |\")\n",
        "print(\"|---------------------|-------------|---------------|----------------|\")\n",
        "print(\"| Logistic regression | Booleans    |     TODO      |      TODO      |\")\n",
        "print(\"| Logistic regression | Frequencies |     TODO      |      TODO      |\")\n",
        "print(\"| Neural network      | Booleans    |     TODO      |      TODO      |\")\n",
        "print(f\"| Neural network      | Frequencies |   {macro_f1_val:.4f}      |    {macro_f1_test:.4f}      |\")  # Update after runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq3e9nEZHrdf"
      },
      "source": [
        "## Optional Extension: CLD3 Embeddings\n",
        "**Advanced**: Replace multihot with learnable embeddings. Average per n-gram type, concat, classify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POs_fg9iHrdf"
      },
      "outputs": [],
      "source": [
        "# Skeleton - TODO: Implement and train. Compare F1.\n",
        "\"\"\"\n",
        "embed_dim = 64\n",
        "class CLD3Model(nn.Module):\n",
        "    def __init__(self, input_dims, num_classes, embed_dim):  # input_dims = MAXES\n",
        "        super().__init__()\n",
        "        self.embed_chars = nn.Embedding(input_dims[0], embed_dim)\n",
        "        self.embed_bigrams = nn.Embedding(input_dims[1], embed_dim)\n",
        "        self.embed_trigrams = nn.Embedding(input_dims[2], embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim * 3, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: multihot (sparse)\n",
        "        # TODO: Split x into chars/bigrams/trigrams\n",
        "        # Get nonzero indices, embed, mean-pool (average embeddings)\n",
        "        # Cat the 3 averages, pass to fc\n",
        "        ...\n",
        "        return self.fc(combined)\n",
        "\n",
        "# Usage: In create_Xy, use multihot (not freq, as embeddings learn weights).\n",
        "# model = CLD3Model(MAXES, len(langs), 64)\n",
        "# Train as before. Expect slight F1 boost (~0.96).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s97nrulBHrdf"
      },
      "source": [
        "## Submission\n",
        "- Complete blanks, run experiments, fill table.\n",
        "- Save notebook, push to your fork.\n",
        "- Report: Table + 1-paragraph discussion (e.g., \"Frequencies outperformed booleans by 1% F1 because...\").\n",
        "\n",
        "Questions? Office hours or Piazza. Happy classifying! üåç"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}